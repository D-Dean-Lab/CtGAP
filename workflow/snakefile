from pathlib import Path
import sys
configfile: "config/config.yaml"

INDIR = Path("path/to/input/")
OUTDIR = Path(config["outdir"])
KRAKENDB = config["krakenDB"]
HUMANREF = config["homoReference"]
CTREF = config["ctReference"]
ADAPTERS = config["adapters"]
OMPABLASTDB = config["ompaBlastDB"]

SAMPLE_NAME, PAIR = glob_wildcards(INDIR / "{sample_name}_{pair}.fastq.gz")
SAMPLES = set(SAMPLE_NAME)
if not SAMPLES:
	print("No input samples were detected. Check the files are named properly: {sample_name}_{pair}")
	sys.exit()

rule all:
	input:
		# trim and qc
		expand(OUTDIR / "status" / "trim.{sample}.txt", sample = SAMPLES),
		expand(OUTDIR / "status" / "scrub.{sample}.txt", sample = SAMPLES),
		expand(OUTDIR / "status" / "trim.scrub.{sample}.txt", sample = SAMPLES),

		# denovo
		expand(OUTDIR / "status" / "normalise.{sample}.txt", sample = SAMPLES),
		expand(OUTDIR / "status" / "shovill.{sample}.txt", sample = SAMPLES),
		#expand(OUTDIR / "status" / "spades.{sample}.txt", sample = SAMPLES),
		expand(OUTDIR / "status" / "blastn.{sample}.txt", sample = SAMPLES),
		
		# alignment 
		expand(OUTDIR / "status" / "aligned.{sample}.txt", sample = SAMPLES),
		
		# collation
		# OUTDIR / "status" / "collate.blast.txt"
		# OUTDIR / "status" / "collage.coverage.txt"
		# OUTDIR / "qc" / "multiqc_report.html",

rule fastp:
	input:
		r1 = INDIR / "{sample}_R1.fastq.gz",
		r2 = INDIR / "{sample}_R2.fastq.gz",
	output:
		r1 = OUTDIR / "{sample}" / "trim" / "{sample}_trim_R1.fastq.gz",
		r2 = OUTDIR / "{sample}" / "trim" / "{sample}_trim_R2.fastq.gz",
		html = OUTDIR / "{sample}" / "trim" /"{sample}.html",
		json = OUTDIR / "{sample}" / "trim" /"{sample}.json",
		status = OUTDIR / "status" / "trim.{sample}.txt",
	params:
		adapters = "resources/adapters.fasta",
		barcodes = "resources/MSS_barcodes.fasta",
		window_size = 1,
		mean_quality = 3,
		right_window_size = 4,
		right_mean_quality=15,
	threads: 12
	conda: "envs/trim.yaml"
	log: OUTDIR / "{sample}" / "log" / "trim.{sample}.log"
	shell:"""
	fastp \
	-i {input.r1} \
	-I {input.r2} \
	-o {output.r1} \
	-O {output.r2} \
	-h {output.html} \
	-j {output.json} \
	--detect_adapter_for_pe \
	--cut_front \
	--cut_front_window_size={params.window_size} \
	--cut_front_mean_quality={params.mean_quality} \
	--cut_tail \
	--cut_tail_window_size={params.window_size} \
	--cut_tail_mean_quality={params.mean_quality} \
	--cut_right \
	--cut_right_window_size={params.right_window_size} \
	--cut_right_mean_quality={params.right_mean_quality} \
	--thread {threads} 2> {log}

	touch {output.status}
	"""

rule scrub:
	input:
		r1 = rules.fastp.output.r1,
		r2 = rules.fastp.output.r2,
	output:
		r1 = OUTDIR / "{sample}" / "scrub" / "{sample}_trim_scrub_R1.fastq.gz",
		r2 = OUTDIR / "{sample}" / "scrub" / "{sample}_trim_scrub_R2.fastq.gz",
		status = OUTDIR / "status" / "scrub.{sample}.txt",
	params:
		db = KRAKENDB,
		human = HUMANREF,
		minlen = 50,
		workdir =  OUTDIR / "{sample}" / "scrub" / "workdir"
	threads: 20
	conda: "envs/scrub.yaml"
	log: OUTDIR / "{sample}" / "log" / "scrub.{sample}.log"
	shell:"""
	scrubby scrub-reads \
	-i {input.r1} {input.r2} \
	-o {output.r1} {output.r2} \
	--kraken-db {params.db} \
	--kraken-taxa "Archaea Eukaryota Holozoa Nucletmycea" \
	--min-len {params.minlen} \
	--minimap2-index {params.human} \
	--workdir {params.workdir} \
	--kraken-threads {threads} 2> {log}	

	touch {output.status}
	"""

rule fastp_qc:
	input:
		r1 = rules.scrub.output.r1,
		r2 = rules.scrub.output.r2,
	output:
		html = OUTDIR / "{sample}" / "scrub" / "{sample}_scrub.html",
		json = OUTDIR / "{sample}" / "scrub" / "{sample}_scrub.json",
		status = OUTDIR / "status" / "trim.scrub.{sample}.txt",
	threads: 12
	conda: "envs/trim.yaml"
	log: OUTDIR / "{sample}" / "log" / "trim.scrub.{sample}.log"
	shell:"""
	fastp \
	-i {input.r1} \
	-I {input.r2} \
	-h {output.html} \
	-j {output.json} \
	--thread {threads} 2> {log}

	touch {output.status}
	"""

rule normalise:
	input:
		r1 = rules.scrub.output.r1,
		r2 = rules.scrub.output.r2,
	output:
		r1 = OUTDIR / "{sample}" / "normalised" / "{sample}_trim_scrub_norm_R1.fastq.gz",
		r2 = OUTDIR / "{sample}" / "normalised" / "{sample}_trim_scrub_norm_R2.fastq.gz",
		status = OUTDIR / "status" / "normalise.{sample}.txt"
	params:
		target = 100,
		minimum = 5,
	conda: "envs/normalise.yaml"
	log: OUTDIR / "{sample}" / "log" / "normalise.{sample}.log"
	threads: 10
	shell:"""
	bbnorm.sh \
	in1={input.r1} \
	in2={input.r2} \
	out1={output.r1} \
	out2={output.r2} \
	target={params.target} \
	threads={threads} \
	min={params.minimum} 2> {log}

	touch {output.status}
	"""

rule shovill:
	input:
		r1 = rules.scrub.output.r1,
		r2 = rules.scrub.output.r2,
	output:
		status = OUTDIR / "status" / "shovill.{sample}.txt",
		outdir = directory(OUTDIR / "{sample}" / "shovill"),
	params:
		contig = OUTDIR / "{sample}" / "shovill" / "contigs.fa",
		gsize = "1.04M",
		depth = 200,
	conda: "envs/shovill.yaml"
	log: OUTDIR / "{sample}" / "log" / "shovill.{sample}.log"
	threads: 8
	shell:"""
	shovill \
	--R1 {input.r1} \
	--R2 {input.r2} \
	--outdir {output.outdir} \
	--gsize {params.gsize} \
	--depth {params.depth} \
	--cpus {threads} 2> {log}

	touch {output.status}
	"""

# are the reads here interleaved? bbsplit outputs r1 and r2.
rule spades:
	input:
		r1 = rules.scrub.output.r1,
		r2 = rules.scrub.output.r2,
	output:
		status = OUTDIR / "status" / "spades.{sample}.txt",
		assembly = directory(OUTDIR / "{sample}" / "spades"),
	threads: 12
	conda: "envs/shovill.yaml"
	log: OUTDIR / "{sample}" / "log" / "spades.{sample}.logs"
	shell:"""
	spades.py \
	--careful \
	--threads {threads} \
	--pe1-1 {params.clean_r1} \
	--pe1-2 {params.clean_r2} \
	-o {output.assembly} 2> {log}
	"""

rule multiqc:
	input:
		status_spades = expand(OUTDIR / "status" / "spades.{sample}.txt", sample = SAMPLES),
		status_shovill = expand(OUTDIR / "status" / "shovill.{sample}.txt", sample = SAMPLES)
	output:
		report = OUTDIR / "multiqc_report.html"
	params:
		fastp_dir = OUTDIR / "qc"
	log: OUTDIR / "log" / "multiqc.log",
	conda: "envs/misc.yaml",
	shell:"""
	multiqc {params.fastp_dir} -n {output.report} 2> {log}
	"""

rule index:
	input:
		reference = CTREF
	output:
		status = OUTDIR / "status" / "ctReference.status"
	params:
		prefix = "resources/ctReference"
	threads: 10
	log: OUTDIR / "log" / "ctReference.index.log"
	conda: "envs/bowtie.yaml"
	shell:"""
	bowtie2-build --threads {threads} {input.reference} {params.prefix} &> {log}

	touch {output.status}
	"""

# why is --local specified here? end to end is generally better
rule bowtie:
	input:
		r1 = rules.scrub.output.r1,
		r2 = rules.scrub.output.r2,
		status = rules.index.output.status,
	output:
		bam = OUTDIR / "{sample}" / "aligned" / "{sample}.bam",
		coverage = OUTDIR / "{sample}" / "coverage.{sample}.tsv",
		status = OUTDIR / "status" / "aligned.{sample}.txt",
	params:
		prefix = rules.index.params.prefix
	threads: 12
	log: OUTDIR / "{sample}" / "log" / "bowtie2.{sample}.log"
	conda: "envs/bowtie.yaml"
	shell:"""
	bowtie2 -x \
	{params.prefix} \
	-1 {input.r1} \
	-2 {input.r2}\
	--threads {threads} | \
	samtools view -bSh - | \
	samtools sort -@{threads} \
	-o {output.bam} & > {log}

	samtools index {output.bam}
	samtools coverage {output.bam} > {output.coverage}

	touch {output.status}
	"""

rule blast_ompa:
	input:
		rules.shovill.params.contig
	output:
		tab = OUTDIR / "{sample}" / "blast" / "blast.ompa.tab",
		status = OUTDIR / "status" / "blastn.{sample}.txt"
	log: OUTDIR / "{sample}" / "log" / "blastompa.{sample}.log"
	params:
		outfmt = 6,
		db = OMPABLASTDB,
		targets = 1
	threads: 5
	conda: "envs/misc.yaml"
	shell:"""
	blastn \
	-query {input.tab} \
	-db {params.db} \
	-max_target_seqs {params.targets} \
	-html  \
	-outfmt {params.outfmt} \
	-out {output}

	touch {output.status}
	"""

rule collate_coverage:
	input:
		status = expand(OUTDIR / "status" / "aligned.{sample}.txt", sample = SAMPLES),
		coverages = expand(OUTDIR / "{sample}" / "coverage.{sample}.tsv", sample = SAMPLES),
	output:
		coverages = OUTDIR / "all.coverage.tsv",
		status = OUTDIR / "status" / "collage.coverage.txt",
	threads: 1
	conda: "envs/misc.yaml"
	shell:"""
	csvtk concat {input.coverages} >  {output.coverages}

	touch {output.status}
	"""
	
rule collage_blast:
	input:
		status_shovill = expand(OUTDIR / "status" / "shovill.{sample}.txt", sample = SAMPLES),
	output:
		tsv = OUTDIR / "all.blast.tsv",
		status = OUTDIR / "status" / "collate.blast.txt",
	params:
		outdir = OUTDIR,
		pattern = "**/blast/*.tab",
	threads: 1
	shell:"""
	grep {params.pattern:q} {params.outdir} > {output.tsv}

	touch {output.status}
	"""


