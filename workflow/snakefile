from pathlib import Path
import sys
configfile: "config/config.yaml"

INDIR = Path("/Users/aaziz/Downloads/ctgap_input")
OUTDIR = Path(config["outdir"])
KRAKENDB = config["krakenDB"]
HUMANREF = config["homoReference"]
CTREF = config["ctReference"]
ADAPTERS = config["adapters"]

SAMPLE_NAME, PAIR = glob_wildcards(INDIR / "{sample_name}_{pair}.fastq.gz")
SAMPLES = set(SAMPLE_NAME)
if not SAMPLES:
	print("No input samples were detected. Check the files are named properly: {sample_name}_{pair}")
	sys.exit()

rule all:
	input:
		expand(OUTDIR / "status" / "trim.{sample}.txt", sample = SAMPLES),
		expand(OUTDIR / "status" / "scrub.{sample}.txt", sample = SAMPLES),
		expand(OUTDIR / "status" / "trim.scrub.{sample}.txt", sample = SAMPLES),
		#expand(OUTDIR / "status" / "normalise.{sample}.txt", sample = SAMPLES),
		expand(OUTDIR / "status" / "shovill.{sample}.txt", sample = SAMPLES),
		# expand(OUTDIR / "status" / "spades.{sample}.txt", sample = SAMPLES),
		# OUTDIR / "qc" / "multiqc_report.html",

rule fastp:
	input:
		r1 = INDIR / "{sample}_R1.fastq.gz",
		r2 = INDIR / "{sample}_R2.fastq.gz",
	output:
		r1 = OUTDIR / "{sample}" / "trim" / "{sample}_trim_R1.fastq.gz",
		r2 = OUTDIR / "{sample}" / "trim" / "{sample}_trim_R2.fastq.gz",
		html = OUTDIR / "{sample}" / "trim" /"{sample}.html",
		json = OUTDIR / "{sample}" / "trim" /"{sample}.json",
		status = OUTDIR / "status" / "trim.{sample}.txt",
	params:
		adapters = "resources/adapters.fasta",
		barcodes = "resources/MSS_barcodes.fasta",
		window_size = 1,
		mean_quality = 3,
		right_window_size = 4,
		right_mean_quality=15,
	threads: 12
	conda: "envs/trim.yaml"
	log: OUTDIR / "log" / "trim.{sample}.log"
	shell:"""
	fastp \
	-i {input.r1} \
	-I {input.r2} \
	-o {output.r1} \
	-O {output.r2} \
	-h {output.html} \
	-j {output.json} \
	--detect_adapter_for_pe \
	--cut_front \
	--cut_front_window_size={params.window_size} \
	--cut_front_mean_quality={params.mean_quality} \
	--cut_tail \
	--cut_tail_window_size={params.window_size} \
	--cut_tail_mean_quality={params.mean_quality} \
	--cut_right \
	--cut_right_window_size={params.right_window_size} \
	--cut_right_mean_quality={params.right_mean_quality} \
	--thread {threads} 2> {log}

	touch {output.status}
	"""

rule scrub:
	input:
		r1 = rules.fastp.output.r1,
		r2 = rules.fastp.output.r2,
	output:
		r1 = OUTDIR / "{sample}" / "scrub" / "{sample}_trim_scrub_R1.fastq.gz",
		r2 = OUTDIR / "{sample}" / "scrub" / "{sample}_trim_scrub_R2.fastq.gz",
		status = OUTDIR / "status" / "scrub.{sample}.txt",
	params:
		db = KRAKENDB,
		human = HUMANREF,
		minlen = 50,
		workdir =  OUTDIR / "{sample}" / "scrub" / "workdir"
	threads: 20
	conda: "envs/scrub.yaml"
	log: OUTDIR / "log" / "{sample}.scrub.log"
	shell:"""
	scrubby scrub-reads \
	-i {input.r1} {input.r2} \
	-o {output.r1} {output.r2} \
	--kraken-db {params.db} \
	--kraken-taxa "Archaea Eukaryota Holozoa Nucletmycea" \
	--min-len {params.minlen} \
	--minimap2-index {params.human} \
	--workdir {params.workdir} \
	--kraken-threads {threads} 2> {log}	

	touch {output.status}
	"""

rule fastp_qc:
	input:
		r1 = rules.scrub.output.r1,
		r2 = rules.scrub.output.r2,
	output:
		html = OUTDIR / "{sample}" / "scrub" / "{sample}_scrub.html",
		json = OUTDIR / "{sample}" / "scrub" / "{sample}_scrub.json",
		status = OUTDIR / "status" / "trim.scrub.{sample}.txt",
	threads: 12
	conda: "envs/trim.yaml"
	log: OUTDIR / "log" / "trim.scrub.{sample}.log"
	shell:"""
	fastp \
	-i {input.r1} \
	-I {input.r2} \
	-h {output.html} \
	-j {output.json} \
	--thread {threads} 2> {log}

	touch {output.status}
	"""

rule normalise:
	input:
		r1 = rules.scrub.output.r1,
		r2 = rules.scrub.output.r2,
	output:
		r1 = OUTDIR / "{sample}" / "normalised" / "{sample}_trim_scrub_norm_R1.fastq.gz",
		r2 = OUTDIR / "{sample}" / "normalised" / "{sample}_trim_scrub_norm_R2.fastq.gz",
		status = OUTDIR / "status" / "normalise.{sample}.txt"
	params:
		target = 100,
		minimum = 5,
	conda: "envs/normalise.yaml"
	log: OUTDIR / "log" / "normalise.{sample}.log"
	threads: 10
	shell:"""
	bbnorm.sh \
	in1={input.r1} \
	in2={input.r2} \
	out1={output.r1} \
	out2={output.r2} \
	target={params.target} \
	threads={threads} \
	min={params.minimum} 2> {log}

	touch {output.status}
	"""

rule shovill:
	input:
		r1 = rules.scrub.output.r1,
		r2 = rules.scrub.output.r2,
	output:
		status = OUTDIR / "status" / "shovill.{sample}.txt",
		outdir = directory(OUTDIR / "{sample}" / "shovill"),
	params:
		gsize = "1.04M",
		depth = 200,
	conda: "envs/shovill.yaml"
	log: OUTDIR / "log" / "shovill.{sample}.log"
	threads: 8
	shell:"""
	shovill \
	--R1 {input.r1} \
	--R2 {input.r2} \
	--outdir {output.outdir} \
	--gsize {params.gsize} \
	--depth {params.depth} \
	--cpus {threads} 2> {log}

	touch {output.status}
	"""

# are the reads here interleaved? bbsplit outputs r1 and r2.
rule spades:
	input:
		r1 = rules.scrub.output.r1,
		r2 = rules.scrub.output.r2,
	output:
		assembly = directory(OUTDIR / "{sample}" / "spades"),
		status = OUTDIR / "status" / "spades.{sample}.txt"
	threads: 12
	conda: "envs/shovill.yaml"
	log: OUTDIR / "log" / "spades.{sample}.logs"
	shell:"""
	spades.py \
	--careful \
	--threads {threads} \
	--pe1-1 {params.clean_r1} \
	--pe1-2 {params.clean_r2} \
	-o {output.assembly} 2> {log}
	"""

rule multiqc:
	input:
		status_spades = expand(OUTDIR / "status" / "spades.{sample}.txt", sample = SAMPLES),
		status_shovill = expand(OUTDIR / "status" / "shovill.{sample}.txt", sample = SAMPLES)
	output:
		report = OUTDIR / "multiqc_report.html"
	params:
		fastp_dir = OUTDIR / "qc"
	log: OUTDIR / "log" / "multiqc.log",
	conda: "envs/misc.yaml",
	shell:"""
	multiqc {params.fastp_dir} -n {output.report} 2> {log}
	"""

rule index:
	input:
		reference = CTREF
	output:
		output1 = "resources/ctReference.1.bt2",
		output2 = "resources/ctReference.2.bt2",
		output3 = "resources/ctReference.3.bt2",
		output4 = "resources/ctReference.4.bt2",
		outputrev1= "resources/ctReference.rev1.bt2",
		outputrev2= "resources/ctReference.rev2.bt2",
		status = OUTDIR / "status" / "ctReference.status"
	params:
		prefix = "resources/ctReference"
	conda: "envs/bowtie.yaml"
	log: OUTDIR / "log" / "ctReference.index.log"
	shell:"""
	bowtie2-build {input.reference} {params.prefix} 2> {log}
	"""

# why is --local specified here? end to end is generally better
rule bowtie:
	input:
		r1 = rules.scrub.output.r1,
		r2 = rules.scrub.output.r2,
		status = rules.index.output.status,
	output:
		bam = OUTDIR / "{sample}" / "aligned" / "{sample}.bam",
		unaligned = OUTDIR / "{sample}" / "aligned" / "{sample}.aligned.sam",
		status = OUTDIR / "status" / "aligned.{sample}.txt",
		coverage = OUTDIR / "{sample}" / "coverage.{sample}.txt"
	threads: 12
	conda: "envs/bowtie.yaml"
	log: OUTDIR / "log" / "bowtie2.{sample}.log"
	shell:"""
	bowtie2 -x \
	{params.prefix} \
	-1 {input.r1} \
	-2 {input.r2}\
	-U {output.unaligned} \
	--threads {threads} | \
	samtools view -bSh - | \
	samtools sort -@{threads} \
	-o {output.bam} 2> {log}

	samtools index {output.bam}
	samtools coverage {output.bam} > {output.converage}

	touch {output.status}
	"""


# rule blast_ompa:
# 	input:
# 		pass
# 	output:
# 		pass
# 	params:
# 		pass
# 	threads: 5
# 	log: OUTDIR / "log" / "blastompa.{sample}.log"
# 	shell:"""
# 	blastn
# 	"""

# rule collate_coverage

# rule collage_blast

